from redis import Redis
from supabase._async.client import AsyncClient
from typing import Dict, List, Any, Tuple, Optional
from app.utils.logger import api_logger
from datetime import date, datetime
import asyncio
import re
import json
import time
import sys
import os
from openai import OpenAI
from app.services.redis_service import RedisService
from app.services.extract_for_file_service import extract_functions_by_type
from app.services.notion_service import NotionService
from app.services.auth_service import get_integration_token
# ë²„í¼ë§ ë¹„í™œì„±í™”
os.environ["PYTHONUNBUFFERED"] = "1"

class CodeAnalysisService:
    """í•¨ìˆ˜ ì¤‘ì‹¬ ì½”ë“œ ë¶„ì„ ë° LLM ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    def __init__(self, redis_client: Redis, supabase: AsyncClient):
        self.redis_client = redis_client
        self.redis_service = RedisService()
        self.supabase = supabase
        self.function_queue = asyncio.Queue()
    
    async def analyze_code_changes(self, files: List[Dict], owner: str, repo: str, commit_sha: str, user_id: str):
        """ì½”ë“œ ë³€ê²½ ë¶„ì„ ì²˜ë¦¬"""
        api_logger.info(f"í•¨ìˆ˜ë³„ ë¶„ì„ ì‹œì‘: {len(files)}ê°œ íŒŒì¼")
        sys.stdout.flush()
        
        for file in files:
            filename = file.get('filename', 'unknown')
            status = file.get('status', '')
                        
            if "patch" not in file and "full_content" not in file:
                api_logger.info(f"íŒŒì¼ '{filename}': ë¶„ì„í•  ë‚´ìš© ì—†ìŒ, ê±´ë„ˆëœ€")
                continue
            
            # íŒŒì¼ ë‚´ìš© ì¶”ì¶œ
            if "full_content" in file:
                file_content = file["full_content"]
                
                # full_contentê°€ patch í˜•íƒœì¸ì§€ í™•ì¸í•˜ê³  íŒŒì‹±
                if (file_content.startswith('@@') or 
                    any(line.startswith(('+', '-', '@@')) for line in file_content.split('\n')[:5])):
                    file_content, _ = self._parse_patch_with_context(file_content)
            else:
                file_content, _ = self._parse_patch_with_context(file["patch"])
            
            # diff ì •ë³´ ì¶”ì¶œ (ìƒˆ íŒŒì¼ì€ diff ë¶„ì„ ë¶ˆí•„ìš”)
            if status == "added":
                diff_info = {}
            elif "patch" in file or (file.get("full_content", "").startswith(('@@', '+', '-'))):
                # patchê°€ ìˆê±°ë‚˜ full_contentê°€ patch í˜•íƒœë©´ diff ì¶”ì¶œ
                patch_content = file.get("patch") or file.get("full_content", "")
                diff_info = self._extract_detailed_diff(patch_content)
            else:
                diff_info = {}
            
            # íŒŒì¼ì„ í•¨ìˆ˜ ë‹¨ìœ„ë¡œ ë¶„í•´
            functions = await self._extract_functions_from_file(file_content, filename, diff_info)
            
            # ê° í•¨ìˆ˜ë¥¼ ë¶„ì„ íì— ì¶”ê°€
            for func_info in functions:
                # ìƒˆ íŒŒì¼ ì²˜ë¦¬
                if status == "added":
                    func_info['has_changes'] = False  # ë³€ê²½ì‚¬í•­ ì•„ë‹˜
                    func_info['changes'] = {}
                    func_info['is_new_file'] = True   # ìƒˆ íŒŒì¼ í”Œë˜ê·¸
                
                await self._enqueue_function_analysis(func_info, commit_sha, user_id, owner, repo)
            
            api_logger.info(f"íŒŒì¼ '{filename}': {len(functions)}ê°œ í•¨ìˆ˜ì¤‘ {len([f for f in functions if f.get('has_changes', True) or f.get('is_new_file', False)])}ê°œ ë³€ê²½ëœ í•¨ìˆ˜ ë¶„ì„ íì— ì¶”ê°€")
            sys.stdout.flush()
    
    def _extract_detailed_diff(self, patch: str) -> Dict[int, Dict]:
        """diff íŒ¨ì¹˜ì—ì„œ ìƒì„¸ ë³€ê²½ ì •ë³´ ì¶”ì¶œ(ë¼ì¸)"""
        changes = {}
        current_line = 0
        
        lines = patch.splitlines()
        i = 0
        
        while i < len(lines):
            line = lines[i]
            
            # @@ -a,b +c,d @@ í˜•ì‹ í—¤ë” ì°¾ê¸°
            hunk_match = re.match(r'^@@ -\d+(?:,\d+)? \+(\d+)(?:,(\d+))? @@', line)
            if hunk_match:
                current_line = int(hunk_match.group(1))
                i += 1
                continue
            
            # ì‚­ì œëœ ë¼ì¸
            if line.startswith('-') and not line.startswith('---'):
                old_code = line[1:]
                # ë‹¤ìŒ ë¼ì¸ì´ ì¶”ê°€ ë¼ì¸ì¸ì§€ í™•ì¸ (ìˆ˜ì •)
                if i + 1 < len(lines) and lines[i + 1].startswith('+'):
                    new_code = lines[i + 1][1:]
                    changes[current_line] = {
                        "type": "modified",
                        "old": old_code,
                        "new": new_code
                    }
                    i += 2  # ë‘ ë¼ì¸ ëª¨ë‘ ì²˜ë¦¬
                    current_line += 1
                else:
                    changes[current_line] = {
                        "type": "deleted",
                        "old": old_code,
                        "new": ""
                    }
                    i += 1
                continue
            
            # ì¶”ê°€ëœ ë¼ì¸
            elif line.startswith('+') and not line.startswith('+++'):
                changes[current_line] = {
                    "type": "added",
                    "old": "",
                    "new": line[1:]
                }
                current_line += 1
                i += 1
                continue
            
            # ì»¨í…ìŠ¤íŠ¸ ë¼ì¸ (ë³€ê²½ ì—†ìŒ)
            else:
                current_line += 1
                i += 1
        
        return changes
    
    def _parse_patch_with_context(self, patch: str) -> Tuple[str, Dict[int, Dict]]:
        """íŒ¨ì¹˜ì—ì„œ ì½”ë“œì™€ ë³€ê²½ ì •ë³´ ë™ì‹œ ì¶”ì¶œ"""
        diff_info = self._extract_detailed_diff(patch)
        
        # íŒ¨ì¹˜ì—ì„œ ìµœì¢… ì½”ë“œ ìƒíƒœ ì¬êµ¬ì„±
        lines = []
        current_line = 1
        
        for patch_line in patch.splitlines():
            if patch_line.startswith(("diff ", "index ", "--- ", "+++ ", "@@")):
                continue
            
            if patch_line.startswith('-'):
                continue  # ì‚­ì œëœ ë¼ì¸ì€ ì œì™¸
            elif patch_line.startswith('+'):
                lines.append(patch_line[1:])  # ì¶”ê°€ëœ ë¼ì¸
            else:
                lines.append(patch_line)  # ì»¨í…ìŠ¤íŠ¸ ë¼ì¸
        
        return '\n'.join(lines), diff_info
    
    async def _extract_functions_from_file(self, file_content: str, filename: str, diff_info: Dict) -> List[Dict]:
        """íŒŒì¼ì—ì„œ í•¨ìˆ˜/ë©”ì„œë“œë¥¼ ê°œë³„ì ìœ¼ë¡œ ì¶”ì¶œ (ìƒˆë¡œìš´ ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒ¨í„´ ì‚¬ìš©)"""
               
        return await extract_functions_by_type(file_content, filename, diff_info)
        
    async def _enqueue_function_analysis(self, func_info: Dict, commit_sha: str, user_id: str, owner: str, repo: str):
        """í•¨ìˆ˜ë³„ ë¶„ì„ ì‘ì—…ì„ íì— ì¶”ê°€ - ë³€ê²½ëœ í•¨ìˆ˜ + ìƒˆ íŒŒì¼"""
        
        # ë³€ê²½ì‚¬í•­ë„ ì—†ê³  ìƒˆ íŒŒì¼ë„ ì•„ë‹ˆë©´ ìŠ¤í‚µ
        if not func_info.get('has_changes', True) and not func_info.get('is_new_file', False): 
            api_logger.info(f"í•¨ìˆ˜ '{func_info['name']}' ë³€ê²½ ì—†ìŒ")
            return
        
        # Redis í‚¤ ìƒì„± (commit_sha í¬í•¨)
        redis_key = f"{user_id}:func:{commit_sha}:{func_info['filename']}:{func_info['name']}"
        cached_result = self.redis_client.get(redis_key)
        
        # ìºì‹œê°€ ìˆê³  ë³€ê²½ì‚¬í•­ì´ ì—†ëŠ” ê¸°ì¡´ íŒŒì¼ë§Œ ìºì‹œ ì‚¬ìš© (ìƒˆ íŒŒì¼ì€ í•­ìƒ ë¶„ì„)
        if cached_result and not func_info.get('has_changes', True) and not func_info.get('is_new_file', False):
            api_logger.info(f"í•¨ìˆ˜ '{func_info['name']}' ë³€ê²½ ì—†ìŒ, ìºì‹œ ì‚¬ìš©")
            return
        
        # ë³€ê²½ëœ í•¨ìˆ˜ì´ê±°ë‚˜ ìƒˆ íŒŒì¼ì¸ ê²½ìš° íì— ì¶”ê°€
        if func_info.get('has_changes', True) or func_info.get('is_new_file', False):
            analysis_item = {
                'function_info': func_info,
                'commit_sha': commit_sha,
                'user_id': user_id,
                'owner': owner,
                'repo': repo,
                'metadata': self._extract_function_metadata(func_info['code'])
            }
            
            await self.function_queue.put(analysis_item)
            
            if func_info.get('is_new_file', False):
                api_logger.info(f"í•¨ìˆ˜ '{func_info['name']}' ë¶„ì„ íì— ì¶”ê°€ë¨ (ìƒˆ íŒŒì¼)")
            else:
                api_logger.info(f"í•¨ìˆ˜ '{func_info['name']}' ë¶„ì„ íì— ì¶”ê°€ë¨ (ë³€ê²½ ê°ì§€)")
    
    def _extract_function_metadata(self, code: str) -> Dict[str, Any]:
        """í•¨ìˆ˜ ì½”ë“œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}
        
        for i, line in enumerate(code.splitlines()[:10]):  # ì²« 10ì¤„ë§Œ ê²€ì‚¬
            line = line.strip()
            if line.startswith('#'):
                # #[ì°¸ì¡°íŒŒì¼.py]{ë¦¬í„´íƒ€ì…}(ìš”êµ¬ì‚¬í•­) í˜•ì‹ íŒŒì‹±
                pattern = r'#\[([^\]]+)\]\{([^}]+)\}\(([^)]+)\)(.*)'
                match = re.match(pattern, line)
                if match:
                    metadata['reference_file'] = match.group(1)
                    metadata['return_type'] = match.group(2)
                    metadata['requirements'] = match.group(3)
                    metadata['custom_prompt'] = match.group(4).strip()
                    break

                # íŒŒì¼#í•¨ìˆ˜ í˜•ì‹: #[íŒŒì¼ê²½ë¡œ#í•¨ìˆ˜ëª…]
                func_ref_match = re.search(r'\[([^#\]]+)#([^\]]+)\]', line)
                if func_ref_match:
                    metadata['reference_file'] = func_ref_match.group(1)
                    metadata['reference_function'] = func_ref_match.group(2)
                    break

                # ë‹¨ìˆœ ì°¸ì¡° íŒŒì¼ë§Œ ìˆëŠ” ê²½ìš°: #[íŒŒì¼.py]
                ref_match = re.search(r'\[([^\]]+\.py)\]', line)
                if ref_match:
                    metadata['reference_file'] = ref_match.group(1)
        
        return metadata
    
    async def process_queue(self):
        """í•¨ìˆ˜ë³„ ë¶„ì„ í ì²˜ë¦¬"""
        api_logger.info("í•¨ìˆ˜ë³„ ë¶„ì„ í ì²˜ë¦¬ ì‹œì‘")
        sys.stdout.flush()
        
        while not self.function_queue.empty():
            item = None
            try:
                item = await self.function_queue.get()
                await self._analyze_function(item)
            except Exception as e:
                api_logger.error(f"í•¨ìˆ˜ ë¶„ì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì‹¤íŒ¨í•œ í•¨ìˆ˜ ì •ë³´ ë¡œê¹…
                if item:
                    func_name = item.get('function_info', {}).get('name', 'unknown')
                    api_logger.error(f"ì‹¤íŒ¨í•œ í•¨ìˆ˜: {func_name}")
            finally:
                # ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ task_done() í˜¸ì¶œ
                if item:
                    self.function_queue.task_done()
        
        api_logger.info("ëª¨ë“  í•¨ìˆ˜ ë¶„ì„ ì™„ë£Œ")
        sys.stdout.flush()
    
    async def _analyze_function(self, item: Dict):
        """ê°œë³„ í•¨ìˆ˜ ë¶„ì„ ì²˜ë¦¬"""
        func_info = item['function_info']
        func_name = func_info['name']
        filename = func_info['filename']
        commit_sha = item['commit_sha']
        user_id = item['user_id']
        
        api_logger.info(f"í•¨ìˆ˜ '{func_name}' ë¶„ì„ ì‹œì‘")
        
        # Redisì—ì„œ ì´ì „ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
        redis_key = f"{user_id}:func:{commit_sha}:{filename}:{func_name}"
        previous_summary = self.redis_client.get(redis_key)
        
        # ì°¸ì¡° íŒŒì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        reference_content = None
        if 'reference_file' in item['metadata']:
            # reference_functionì´ ìˆìœ¼ë©´ íŒŒì¼#í•¨ìˆ˜ í˜•ì‹ìœ¼ë¡œ ì¡°í•©
            if 'reference_function' in item['metadata']:
                reference_path = f"{item['metadata']['reference_file']}#{item['metadata']['reference_function']}"
            else:
                reference_path = item['metadata']['reference_file']
                
            reference_content = await self._fetch_reference_function(
                reference_path,
                item['owner'], 
                item['repo'], 
                item['commit_sha'],
                user_id
            )
        
        # í•¨ìˆ˜ê°€ ê¸¸ë©´ ì²­í¬ë¡œ ë¶„í• 
        chunks = self._split_function_if_needed(func_info['code'])
        
        if len(chunks) == 1:
            # ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬
            summary = await self._call_llm_for_function(
                func_info, 
                chunks[0], 
                item['metadata'], 
                previous_summary, 
                reference_content
            )
        else:
            # ë‹¤ì¤‘ ì²­í¬ ì—°ì† ì²˜ë¦¬
            summary = await self._process_multi_chunk_function(
                func_info, 
                chunks, 
                item['metadata'], 
                previous_summary, 
                reference_content
            )
        
        # Redisì— ìµœì¢… ìš”ì•½ ì €ì¥ (strì„ bytesë¡œ ì¸ì½”ë”©)
        summary_bytes = summary.encode('utf-8') if isinstance(summary, str) else summary
        self.redis_client.setex(redis_key, 86400 * 7, summary_bytes)  # 7ì¼ ë³´ê´€
        
        # Notion ì—…ë°ì´íŠ¸ëŠ” íŒŒì¼ ë‹¨ìœ„ë¡œ ë³„ë„ ì²˜ë¦¬
        await self._update_notion_if_needed(func_info, summary, user_id, commit_sha)
        
        api_logger.info(f"í•¨ìˆ˜ '{func_name}' ë¶„ì„ ì™„ë£Œ")
    
    def _split_function_if_needed(self, code: str, max_length: int = 2000) -> List[str]:
        """í•¨ìˆ˜ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²­í¬ë¡œ ë¶„í• """
        if len(code) <= max_length:
            return [code]
        
        # ë‹¨ìˆœ ê¸¸ì´ ê¸°ë°˜ ë¶„í•  (ë³µì¡í•œ ì •ê·œì‹ ì œê±°)
        chunks = []
        for i in range(0, len(code), max_length):
            chunks.append(code[i:i + max_length])
        
        return chunks
    
    async def _process_multi_chunk_function(self, func_info: Dict, chunks: List[str], 
                                          metadata: Dict, previous_summary: str, 
                                          reference_content: str) -> str:
        """ë‹¤ì¤‘ ì²­í¬ í•¨ìˆ˜ì˜ ì—°ì†ì  ìš”ì•½ ì²˜ë¦¬"""
        current_summary = previous_summary
        
        for i, chunk in enumerate(chunks):
            api_logger.info(f"í•¨ìˆ˜ '{func_info['name']}' ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬")
            sys.stdout.flush()
            
            # ì´ì „ ìš”ì•½ì„ í¬í•¨í•œ LLM í˜¸ì¶œ
            chunk_summary = await self._call_llm_for_function(
                func_info, 
                chunk, 
                metadata, 
                current_summary,  # ì´ì „ ìš”ì•½ í¬í•¨
                reference_content,
                chunk_index=i,
                total_chunks=len(chunks)
            )
            
            current_summary = chunk_summary  # ë‹¤ìŒ ì²­í¬ì—ì„œ ì‚¬ìš©í•  ìš”ì•½ ì—…ë°ì´íŠ¸
        
        return current_summary
    
    async def _call_llm_for_function(self, func_info: Dict, code: str, metadata: Dict, 
                                   previous_summary: str = None, reference_content: str = None,
                                   chunk_index: int = 0, total_chunks: int = 1) -> str:
        """í•¨ìˆ˜ë³„ LLM ë¶„ì„ í˜¸ì¶œ"""
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_parts = []
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        if total_chunks > 1:
            prompt_parts.append(f"ë‹¤ìŒì€ '{func_info['name']}' í•¨ìˆ˜ì˜ {chunk_index+1}/{total_chunks} ì²­í¬ì…ë‹ˆë‹¤.")
        else:
            prompt_parts.append(f"ë‹¤ìŒì€ '{func_info['name']}' í•¨ìˆ˜ì˜ ì½”ë“œì…ë‹ˆë‹¤.")
        
        # ì´ì „ ìš”ì•½ì´ ìˆìœ¼ë©´ í¬í•¨
        if previous_summary:
            prompt_parts.append(f"\nì´ì „ ë¶„ì„ ê²°ê³¼:\n{previous_summary}")
            if total_chunks > 1:
                prompt_parts.append("\nìœ„ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì½”ë“œ ì²­í¬ë¥¼ ë¶„ì„í•˜ê³  í†µí•©ëœ ìš”ì•½ì„ ì œê³µí•˜ì„¸ìš”.")
            else:
                prompt_parts.append("\nìœ„ ë¶„ì„ì„ ì°¸ê³ í•˜ì—¬ ë³€ê²½ì‚¬í•­ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.")
        
        # ì°¸ì¡° íŒŒì¼ ë‚´ìš© í¬í•¨
        if reference_content:
            prompt_parts.append(f"\nì°¸ì¡° í•¨ìˆ˜ ì½”ë“œ:\n{reference_content}")
        
        # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
        if 'custom_prompt' in metadata:
            prompt_parts.append(f"\nì¶”ê°€ ìš”êµ¬ì‚¬í•­: {metadata['custom_prompt']}")
        
        if 'return_type' in metadata:
            prompt_parts.append(f"\nì˜ˆìƒ ë°˜í™˜ íƒ€ì…: {metadata['return_type']}")
        
        if 'requirements' in metadata:
            prompt_parts.append(f"\nêµ¬í˜„ ìš”êµ¬ì‚¬í•­: {metadata['requirements']}")
        
        # ë³€ê²½ ì‚¬í•­ì´ ìˆìœ¼ë©´ ê°•ì¡°
        if func_info.get('has_changes', False):
            changes_text = []
            for line_num, change in func_info.get('changes', {}).items():
                if change['type'] == 'modified':
                    changes_text.append(f"ë¼ì¸ {line_num}: '{change['old']}' â†’ '{change['new']}'")
                elif change['type'] == 'added':
                    changes_text.append(f"ë¼ì¸ {line_num}: ì¶”ê°€ë¨ - '{change['new']}'")
                elif change['type'] == 'deleted':
                    changes_text.append(f"ë¼ì¸ {line_num}: ì‚­ì œë¨ - '{change['old']}'")
            
            if changes_text:
                prompt_parts.append(f"\nğŸ”¥ ì£¼ìš” ë³€ê²½ì‚¬í•­:\n" + "\n".join(changes_text))
                prompt_parts.append("\níŠ¹íˆ ìœ„ ë³€ê²½ì‚¬í•­ì˜ ëª©ì ê³¼ ì˜í–¥ì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        
        # ë¶„ì„í•  ì½”ë“œ
        prompt_parts.append(f"\në¶„ì„í•  ì½”ë“œ:\n```{func_info.get('filename', '').split('.')[-1]}\n{code}\n```")
        
        # ì‘ë‹µ í˜•ì‹ ì§€ì •
        prompt_parts.append("""
ë¶„ì„ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”:
1. **ê¸°ëŠ¥ ìš”ì•½**: í•¨ìˆ˜ì˜ í•µì‹¬ ëª©ì ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ
2. **ì£¼ìš” ë¡œì§**: í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ì´ë‚˜ ì²˜ë¦¬ íë¦„
3. **ë³€ê²½ ì˜í–¥**: (ë³€ê²½ì‚¬í•­ì´ ìˆëŠ” ê²½ìš°) ë³€ê²½ìœ¼ë¡œ ì¸í•œ ë™ì‘ ë³€í™”
4. **ì˜ì¡´ì„±**: ì‚¬ìš©í•˜ëŠ” ì™¸ë¶€ í•¨ìˆ˜ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
5. **ê°œì„  ì œì•ˆ**: (í•„ìš”ì‹œ) ì½”ë“œ í’ˆì§ˆ í–¥ìƒ ë°©ì•ˆ
""")
        
        full_prompt = "\n".join(prompt_parts)
        
        # ì„ì‹œ ì‘ë‹µ ë°˜í™˜ (ì‹¤ì œ LLM í˜¸ì¶œ ëŒ€ì‹ )
        return f"[íŒŒì‹± ì™„ë£Œ] {func_info['name']} í•¨ìˆ˜ ë¶„ì„ ì •ë³´ ë¡œê¹…ë¨"
    
    async def _fetch_reference_function(self, reference_file: str, owner: str, repo: str, commit_sha: str, user_id: str) -> str:
        """ì°¸ì¡° íŒŒì¼ì˜ í•¨ìˆ˜ ìš”ì•½ì„ Redisì—ì„œ ì¡°íšŒ"""
        # íŒŒì¼ì—ì„œ íŠ¹ì • í•¨ìˆ˜ê°€ ì§€ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if '#' in reference_file:
            file_path, func_name = reference_file.split('#', 1)
            redis_key = f"{user_id}:func:{commit_sha}:{file_path}:{func_name}"
            cached_content = self.redis_client.get(redis_key)
            if cached_content:
                return cached_content
        else:
            # íŒŒì¼ ì „ì²´ ì°¸ì¡°ì¸ ê²½ìš° ì£¼ìš” í•¨ìˆ˜ë“¤ ì¡°íšŒ
            pattern = f"{user_id}:func:{commit_sha}:{reference_file}:*"
            function_keys = self.redis_client.keys(pattern)
            
            if function_keys:
                # ì—¬ëŸ¬ í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ ëª¨ë‘ ì¡°í•©
                all_summaries = []
                for key in function_keys:
                    # Redis keyê°€ bytesì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ strë¡œ ë³€í™˜
                    key_str = key.decode('utf-8') if isinstance(key, bytes) else key
                    func_name = key_str.split(":")[-1]
                    summary_raw = self.redis_client.get(key)
                    if summary_raw:
                        # bytesë©´ strë¡œ ë³€í™˜, ì´ë¯¸ strì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        summary = summary_raw.decode('utf-8') if isinstance(summary_raw, bytes) else summary_raw
                        all_summaries.append(f"**{func_name}():**\n{summary}")
                
                if all_summaries:
                    return "\n\n".join(all_summaries)
        
        # Redisì— ì—†ìœ¼ë©´ íŒŒì¼ ë‚´ìš© ìš”ì²­ (ê¸°ì¡´ ë°©ì‹ í™œìš©)
        return await self._request_reference_file_content(reference_file, owner, repo, commit_sha)
    
    async def _request_reference_file_content(self, reference_file: str, owner: str, repo: str, commit_sha: str) -> str:
        """GitHubì—ì„œ ì°¸ì¡° íŒŒì¼ ë‚´ìš© ìš”ì²­ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)"""
        # ê¸°ì¡´ êµ¬í˜„ê³¼ ë™ì¼í•œ Redis í‚¤-ê°’ ë°©ì‹ ì‚¬ìš©
        request_id = f"ref_{int(time.time())}_{hash(reference_file) % 1000}"
        
        request_data = {
            'path': reference_file,
            'commit_sha': commit_sha,
            'request_id': request_id
        }
        
        request_key = f"ref_request:{owner}:{repo}:{request_id}"
        response_key = f"ref_response:{owner}:{repo}:{request_id}"
        
        # JSON ë°ì´í„°ë¥¼ bytesë¡œ ì¸ì½”ë”©í•´ì„œ ì €ì¥
        request_data_json = json.dumps(request_data)
        request_data_bytes = request_data_json.encode('utf-8')
        self.redis_client.setex(request_key, 300, request_data_bytes)
        
        # 5ì´ˆ í´ë§ ëŒ€ê¸°
        for _ in range(10):
            response_str = self.redis_client.get(response_key)
            if response_str:
                response_data = json.loads(response_str)
                if response_data.get('status') == 'success':
                    return response_data.get('content', '')
            await asyncio.sleep(0.5)
        
        return ""
    
    async def _update_notion_if_needed(self, func_info: Dict, summary: str, user_id: str, commit_sha: str):
        """íŒŒì¼ë³„ ì¢…í•© ë¶„ì„ ë° Notion ì—…ë°ì´íŠ¸"""
        filename = func_info['filename']
        
        # 1. íŒŒì¼ì˜ ëª¨ë“  í•¨ìˆ˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if await self._is_file_analysis_complete(filename, user_id):
            # 2. íŒŒì¼ë³„ ì¢…í•© ë¶„ì„ ìˆ˜í–‰
            file_summary = await self._generate_file_level_analysis(filename, user_id)
            
            # 3. Notion AI ìš”ì•½ ë¸”ë¡ ì—…ë°ì´íŠ¸
            await self._update_notion_ai_block(filename, file_summary, user_id, commit_sha)
            
            # 4. ì•„í‚¤í…ì²˜ ê°œì„  ì œì•ˆ ìƒì„±
            await self._generate_architecture_suggestions(filename, file_summary, user_id)

    async def _is_file_analysis_complete(self, filename: str, user_id: str) -> bool:
        """íŒŒì¼ì˜ ëª¨ë“  í•¨ìˆ˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        
        # Redisì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ ëª¨ë“  í•¨ìˆ˜ í‚¤ ì¡°íšŒ
        pattern = f"{user_id}:func:*:{filename}:*"
        function_keys = self.redis_client.keys(pattern)
        
        # ë¶„ì„ ëŒ€ê¸° ì¤‘ì¸ í•¨ìˆ˜ê°€ ìˆëŠ”ì§€ íì—ì„œ í™•ì¸
        temp_queue = []
        pending_functions = set()
        
        # íì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ ëŒ€ê¸° ì¤‘ì¸ í•¨ìˆ˜ë“¤ í™•ì¸
        try:
            while not self.function_queue.empty():
                item = await self.function_queue.get()
                temp_queue.append(item)
                
                if item and item.get('function_info', {}).get('filename') == filename:
                    func_name = item.get('function_info', {}).get('name', 'unknown')
                    pending_functions.add(func_name)
        except Exception as e:
            api_logger.error(f"í í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            # íì— ë‹¤ì‹œ ë„£ê¸°
            for item in temp_queue:
                if item:  # None ì²´í¬ ì¶”ê°€
                    await self.function_queue.put(item)
        
        # ëŒ€ê¸° ì¤‘ì¸ í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ ì™„ë£Œ
        is_complete = len(pending_functions) == 0
        
        if is_complete:
            api_logger.info(f"íŒŒì¼ '{filename}' ë¶„ì„ ì™„ë£Œ í™•ì¸ë¨")
        else:
            api_logger.info(f"íŒŒì¼ '{filename}' ëŒ€ê¸° ì¤‘ì¸ í•¨ìˆ˜: {pending_functions}")
        
        return is_complete

    async def _generate_file_level_analysis(self, filename: str, user_id: str) -> str:
        """íŒŒì¼ ì „ì²´ íë¦„ ë¶„ì„ ë° ì¢…í•© ìš”ì•½ ìƒì„±"""
        
        # 1. íŒŒì¼ì˜ ëª¨ë“  í•¨ìˆ˜ ìš”ì•½ ìˆ˜ì§‘
        pattern = f"{user_id}:func:*:{filename}:*"
        function_keys = self.redis_client.keys(pattern)
        
        function_summaries = {}
        for key in function_keys:
            # Redis keyê°€ bytesì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ strë¡œ ë³€í™˜
            key_str = key.decode('utf-8') if isinstance(key, bytes) else key
            function_name = key_str.split(":")[-1]  # func:íŒŒì¼:í•¨ìˆ˜ëª… â†’ í•¨ìˆ˜ëª…
            
            summary_raw = self.redis_client.get(key)
            if summary_raw:
                # bytesë©´ strë¡œ ë³€í™˜, ì´ë¯¸ strì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                summary = summary_raw.decode('utf-8') if isinstance(summary_raw, bytes) else summary_raw
                function_summaries[function_name] = summary
        
        # 2. í•¨ìˆ˜ë“¤ì„ íƒ€ì…ë³„ë¡œ ë¶„ë¥˜
        categorized_functions = {
            'global': [],           # ì „ì—­ ì½”ë“œ
            'class_methods': {},    # í´ë˜ìŠ¤ë³„ ë©”ì„œë“œ ê·¸ë£¹
            'functions': [],        # ë…ë¦½ í•¨ìˆ˜
            'helpers': []          # í—¬í¼ í•¨ìˆ˜
        }
        
        for func_name, summary in function_summaries.items():
            if func_name == 'globals_and_imports':
                categorized_functions['global'].append(summary)
            elif '.' in func_name:  # í´ë˜ìŠ¤.ë©”ì„œë“œ í˜•ì‹
                class_name, method_name = func_name.split('.', 1)
                if class_name not in categorized_functions['class_methods']:
                    categorized_functions['class_methods'][class_name] = []
                categorized_functions['class_methods'][class_name].append({
                    'method': method_name,
                    'summary': summary
                })
            elif func_name.startswith('_'):
                categorized_functions['helpers'].append({
                    'function': func_name,
                    'summary': summary
                })
            else:
                categorized_functions['functions'].append({
                    'function': func_name,
                    'summary': summary
                })
        
        # 3. íŒŒì¼ ì „ì²´ ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        analysis_prompt = f"""
    íŒŒì¼ëª…: {filename}

    ë‹¤ìŒì€ ì´ íŒŒì¼ì˜ ëª¨ë“  í•¨ìˆ˜ë³„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤. 
    ì „ì²´ì ì¸ ì•„í‚¤í…ì²˜ íë¦„ê³¼ ê°œì„ ë°©ì•ˆì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

    ## ğŸ“‹ í•¨ìˆ˜ë³„ ë¶„ì„ ê²°ê³¼

    ### ğŸŒ ì „ì—­ ì½”ë“œ (ì„í¬íŠ¸/ìƒìˆ˜)
    {categorized_functions['global'][0] if categorized_functions['global'] else 'ì—†ìŒ'}

    """

        # í´ë˜ìŠ¤ë³„ ë©”ì„œë“œ ì¶”ê°€
        for class_name, methods in categorized_functions['class_methods'].items():
            analysis_prompt += f"""
    ### ğŸ—ï¸ {class_name} í´ë˜ìŠ¤
    """
            for method in methods:
                analysis_prompt += f"""
    **{method['method']}():**
    {method['summary']}

    """

        # ë…ë¦½ í•¨ìˆ˜ë“¤ ì¶”ê°€
        if categorized_functions['functions']:
            analysis_prompt += """
    ### âš¡ ë…ë¦½ í•¨ìˆ˜ë“¤
    """
            for func in categorized_functions['functions']:
                analysis_prompt += f"""
    **{func['function']}():**
    {func['summary']}

    """

        # í—¬í¼ í•¨ìˆ˜ë“¤ ì¶”ê°€
        if categorized_functions['helpers']:
            analysis_prompt += """
    ### ğŸ”§ í—¬í¼ í•¨ìˆ˜ë“¤
    """
            for helper in categorized_functions['helpers']:
                analysis_prompt += f"""
    **{helper['function']}():**
    {helper['summary']}

    """

        # ë¶„ì„ ìš”ì²­ ì¶”ê°€
        analysis_prompt += """
    ## ğŸ¯ ì „ì²´ ë¶„ì„ ìš”ì²­

    ë‹¤ìŒ ê´€ì ì—ì„œ ì¢…í•© ë¶„ì„í•´ì£¼ì„¸ìš”:

    ### 1. **ğŸ›ï¸ ì•„í‚¤í…ì²˜ ë¶„ì„**
    - ì „ì²´ì ì¸ ì„¤ê³„ íŒ¨í„´ê³¼ êµ¬ì¡°
    - í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ë“¤ ê°„ì˜ ê´€ê³„
    - ì±…ì„ ë¶„ë¦¬ê°€ ì˜ ë˜ì–´ìˆëŠ”ì§€

    ### 2. **ğŸ”„ ë°ì´í„° íë¦„ ë¶„ì„**  
    - ì£¼ìš” ë°ì´í„°ê°€ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ëŠ”ì§€
    - í•¨ìˆ˜ë“¤ ê°„ì˜ í˜¸ì¶œ ê´€ê³„ì™€ ì˜ì¡´ì„±
    - ë³‘ëª© êµ¬ê°„ì´ë‚˜ ê°œì„  í¬ì¸íŠ¸

    ### 3. **ğŸš€ ì„±ëŠ¥ ë° í™•ì¥ì„±**
    - ì„±ëŠ¥ìƒ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„
    - í™•ì¥ì„±ì„ ìœ„í•œ ê°œì„  ë°©ì•ˆ
    - ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™” í¬ì¸íŠ¸

    ### 4. **ğŸ›¡ï¸ ì•ˆì •ì„± ë° ì—ëŸ¬ ì²˜ë¦¬**
    - ì˜ˆì™¸ ì²˜ë¦¬ê°€ ì¶©ë¶„í•œì§€
    - ì—£ì§€ ì¼€ì´ìŠ¤ ëŒ€ì‘ ë°©ì•ˆ
    - ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ê°œì„ ì 

    ### 5. **ğŸ“ˆ ì½”ë“œ í’ˆì§ˆ í‰ê°€**
    - ê°€ë…ì„± ë° ìœ ì§€ë³´ìˆ˜ì„±
    - ì¤‘ë³µ ì½”ë“œë‚˜ ë¦¬íŒ©í† ë§ ëŒ€ìƒ
    - í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„±

    ### 6. **ğŸ¯ êµ¬ì²´ì  ê°œì„  ì œì•ˆ**
    - ìš°ì„ ìˆœìœ„ë³„ ê°œì„  ì‚¬í•­ (ìƒ/ì¤‘/í•˜)
    - ê° ê°œì„ ì‚¬í•­ì˜ ì˜ˆìƒ íš¨ê³¼
    - êµ¬í˜„ ë‚œì´ë„ ë° ì†Œìš” ì‹œê°„ ì¶”ì •

    **ì‘ë‹µ í˜•ì‹:** ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì—¬ Notionì—ì„œ ì½ê¸° ì¢‹ê²Œ ì‘ì„±
    """
        # 4. LLM í˜¸ì¶œí•˜ì—¬ ì¢…í•© ë¶„ì„
        file_analysis = await self._call_llm_for_file_analysis(analysis_prompt)
        
        # 5. ë¶„ì„ ê²°ê³¼ë¥¼ Redisì— ìºì‹± (íŒŒì¼ ë‹¨ìœ„)
        file_cache_key = f"{user_id}:file_analysis:{filename}"
        file_analysis_bytes = file_analysis.encode('utf-8') if isinstance(file_analysis, str) else file_analysis
        self.redis_client.setex(file_cache_key, 86400 * 3, file_analysis_bytes)  # 3ì¼ ë³´ê´€
        
        return file_analysis

    async def _call_llm_for_file_analysis(self, prompt: str) -> str:
        """íŒŒì¼ ì „ì²´ ë¶„ì„ì„ ìœ„í•œ LLM í˜¸ì¶œ"""
        # ì„ì‹œë¡œ LLM í˜¸ì¶œ ë¹„í™œì„±í™” - ë””ë²„ê¹…ìš©
        api_logger.info("íŒŒì¼ ë¶„ì„ LLM í˜¸ì¶œ ì™„ë£Œ (ë”ë¯¸ ì‘ë‹µ)")
        # ë”ë¯¸ ì‘ë‹µ ë°˜í™˜ (ì‹¤ì œ LLM í˜¸ì¶œ ëŒ€ì‹ )
        dummy_response = f"""
## ğŸ›ï¸ ì•„í‚¤í…ì²˜ ë¶„ì„
íŒŒì¼ ì „ì²´ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ

## ğŸ”„ ë°ì´í„° íë¦„ ë¶„ì„  
í•¨ìˆ˜ê°„ í˜¸ì¶œ ê´€ê³„ ë¶„ì„ ì™„ë£Œ

## ğŸš€ ì„±ëŠ¥ ë° í™•ì¥ì„±
ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸ ë¶„ì„ ì™„ë£Œ

## ğŸ›¡ï¸ ì•ˆì •ì„± ë° ì—ëŸ¬ ì²˜ë¦¬
ì˜ˆì™¸ ì²˜ë¦¬ ë¶„ì„ ì™„ë£Œ

## ğŸ“ˆ ì½”ë“œ í’ˆì§ˆ í‰ê°€
ì½”ë“œ í’ˆì§ˆ í‰ê°€ ì™„ë£Œ

## ğŸ¯ êµ¬ì²´ì  ê°œì„  ì œì•ˆ
- ìš°ì„ ìˆœìœ„ë³„ ê°œì„ ì‚¬í•­ ë¶„ì„ ì™„ë£Œ
"""
        
        return dummy_response
    
    def _find_closest_page_to_today(self, pages: list) -> dict | None:
        """
        ê°€ì¥ ê°€ê¹Œìš´ ë‚ ì§œì— ìƒì„±ëœ rowì— ìš”ì•½ ì €ì¥
        """
        today = date.today()
        
        if not pages:
            return None
        
        # ì˜¤ëŠ˜ ë‚ ì§œì™€ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ í˜ì´ì§€ ì°¾ê¸°
        closest_page = None
        min_diff = float('inf')
        
        for page in pages:
            page_date = datetime.fromisoformat(page["date"]).date()
            diff = abs((today - page_date).days)
            
            if diff < min_diff:
                min_diff = diff
                closest_page = page
        
        return closest_page
    
    def _collect_function_summaries(self, user_id: str, filename: str) -> Dict[str, str]:
        """Redisì—ì„œ íŒŒì¼ì˜ í•¨ìˆ˜ë³„ ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘"""
        func_keys = self.redis_client.keys(f"{user_id}:func:*:{filename}:*")
        func_summaries = {}
        
        for key in func_keys:
            # Redis keyê°€ bytesì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ strë¡œ ë³€í™˜
            key_str = key.decode('utf-8') if isinstance(key, bytes) else key
            # key í˜•ì‹: "{user_id}:func:{commit_sha}:{filename}:{func_name}"
            func_name = key_str.split(":")[-1]  # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ í•¨ìˆ˜ëª…
            summary_raw = self.redis_client.get(key)
            if summary_raw:
                # bytesë©´ strë¡œ ë³€í™˜, ì´ë¯¸ strì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                summary = summary_raw.decode('utf-8') if isinstance(summary_raw, bytes) else summary_raw
                func_summaries[func_name] = summary
        
        api_logger.info(f"íŒŒì¼ '{filename}': {len(func_summaries)}ê°œ í•¨ìˆ˜ ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘")
        return func_summaries
    
    def _build_analysis_summary(self, filename: str, file_summary: str, func_summaries: Dict[str, str]) -> str:
        """í† ê¸€ ë¸”ë¡ ë‚´ë¶€ì— ë“¤ì–´ê°ˆ ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸  êµ¬ì„±"""
        analysis_parts = [
            f"**{filename} ì „ì²´**\\n",
            file_summary,
            ""
        ]
        
        # í•¨ìˆ˜ë³„ í‰ê°€ ì¶”ê°€
        for func_name, summary in func_summaries.items():
            analysis_parts.extend([
                f"**{func_name}()**\\n",
                summary,
                ""
            ])
        
        return "\n".join(analysis_parts)
    
    async def _find_target_page(self, user_id: str) -> Optional[Dict]:
        """í˜„ì¬ í™œì„± DBì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë‚ ì§œì˜ í•™ìŠµ í˜ì´ì§€ ì°¾ê¸°"""
        # 1. í˜„ì¬ í™œì„± DB ì°¾ê¸° (Redis â†’ Supabase ìˆœ)
        curr_db_id = await self.redis_service.get_default_db(user_id, self.redis_client)
        if not curr_db_id:
            db_result = self.supabase.table("db_webhooks")\
                .select("learning_db_id")\
                .eq("created_by", user_id)\
                .execute()
            
            if not db_result.data:
                api_logger.error(f"í˜„ì¬ ì‚¬ìš©ì¤‘ì¸ í•™ìŠµ DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            curr_db_id = db_result.data[0]["learning_db_id"]
        
        # 2. í•´ë‹¹ DBì˜ í˜ì´ì§€ë“¤ ì°¾ê¸° (Redis â†’ Supabase ìˆœ) 
        pages = await self.redis_service.get_db_pages(user_id, curr_db_id, self.redis_client)
        if not pages:
            pages_result = self.supabase.table("learning_pages")\
                .select("*")\
                .eq("learning_db_id", curr_db_id)\
                .execute()
            pages = pages_result.data
        
        # 3. ê°€ì¥ ê°€ê¹Œìš´ ë‚ ì§œì˜ í˜ì´ì§€ ì„ íƒ
        closest_page = self._find_closest_page_to_today(pages)
        if not closest_page:
            api_logger.error(f"ìµœê·¼ í•™ìŠµ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        return closest_page

    #[app.utils.notion_utils.py#markdown_to_notion_blocks]{}
    async def _append_analysis_to_notion(self, ai_analysis_log_page_id: str, analysis_summary: str, commit_sha: str, user_id: str):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ì œëª©3 í† ê¸€ ë¸”ë¡ìœ¼ë¡œ ë…¸ì…˜ì— ì¶”ê°€"""
        # 1. Notion í† í° ì¡°íšŒ
        redis_service = RedisService()
        token = await redis_service.get_token(user_id, self.redis_client)

        if not token:
            # Redisì— ì—†ìœ¼ë©´ Supabaseì—ì„œ ì¡°íšŒ
            token = await get_integration_token(user_id=user_id, provider="notion", supabase=self.supabase)
            if token:
                # ì¡°íšŒí•œ í† í°ì„ Redisì— ì €ì¥ (1ì‹œê°„ ë§Œë£Œ)
                await redis_service.set_token(user_id, token, self.redis_client, expire_seconds=3600)

                
        if not token:
            api_logger.error(f"Notion í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {user_id}")
            return
        
        # 2. NotionServiceë¡œ ìš”ì²­ ì „ì†¡
        notion_service = NotionService(token=token)
        await notion_service.append_code_analysis_to_page(
            ai_analysis_log_page_id, 
            analysis_summary, 
            commit_sha
        )
        
        api_logger.info(f"Notionì— ë¶„ì„ ê²°ê³¼ ì¶”ê°€ ì™„ë£Œ: {commit_sha[:8]}")

    async def _update_notion_ai_block(self, filename: str, file_summary: str, user_id: str, commit_sha: str):
        """Notion AI ìš”ì•½ ë¸”ë¡ ì—…ë°ì´íŠ¸"""
        try:
            api_logger.info(f"íŒŒì¼ '{filename}' Notion ì—…ë°ì´íŠ¸ ì‹œì‘")
            sys.stdout.flush()
            
            # 1. í•¨ìˆ˜ë³„ ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
            func_summaries = self._collect_function_summaries(user_id, filename)
            
            # 2. ë¶„ì„ ìš”ì•½ êµ¬ì„±
            analysis_summary = self._build_analysis_summary(filename, file_summary, func_summaries)
            
            # 3. íƒ€ê²Ÿ í˜ì´ì§€ ì°¾ê¸°
            target_page = await self._find_target_page(user_id)
            if not target_page:
                api_logger.error(f"íƒ€ê²Ÿ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
                
            # 4. ì œëª©3 í† ê¸€ ë¸”ë¡ ìƒì„± ë° ì¶”ê°€
            await self._append_analysis_to_notion(
                target_page["ai_block_id"], 
                analysis_summary, 
                commit_sha,
                user_id
            )
            
            api_logger.info(f"íŒŒì¼ '{filename}' Notion ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            sys.stdout.flush()
            
        except Exception as e:
            api_logger.error(f"Notion ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            sys.stdout.flush()

    async def _generate_architecture_suggestions(self, filename: str, file_summary: str, user_id: str):
        """ì•„í‚¤í…ì²˜ ê°œì„  ì œì•ˆ ìƒì„± ë° ë³„ë„ ì €ì¥"""
        
        # ê°œì„  ì œì•ˆë§Œ ì¶”ì¶œí•˜ëŠ” LLM í˜¸ì¶œ
        suggestions_prompt = f"""
    ë‹¤ìŒ íŒŒì¼ ë¶„ì„ ê²°ê³¼ì—ì„œ **êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ì œì•ˆ**ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

    {file_summary}

    í˜•ì‹:
    ## ğŸš€ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (1-2ì‹œê°„)
    - [ ] êµ¬ì²´ì  ê°œì„ ì‚¬í•­ 1
    - [ ] êµ¬ì²´ì  ê°œì„ ì‚¬í•­ 2

    ## ğŸ”§ ë‹¨ê¸° ê°œì„  (1ì£¼ ì´ë‚´)  
    - [ ] êµ¬ì²´ì  ê°œì„ ì‚¬í•­ 3
    - [ ] êµ¬ì²´ì  ê°œì„ ì‚¬í•­ 4

    ## ğŸ’¡ ì¥ê¸° ê°œì„  (1ê°œì›” ì´ë‚´)
    - [ ] êµ¬ì²´ì  ê°œì„ ì‚¬í•­ 5
    - [ ] êµ¬ì²´ì  ê°œì„ ì‚¬í•­ 6
    """
        
        suggestions = await self._call_llm_for_file_analysis(suggestions_prompt)
        
        # Redisì— ê°œì„  ì œì•ˆ ë³„ë„ ì €ì¥ (strì„ bytesë¡œ ì¸ì½”ë”©)
        suggestions_key = f"{user_id}:suggestions:{filename}"
        suggestions_bytes = suggestions.encode('utf-8') if isinstance(suggestions, str) else suggestions
        self.redis_client.setex(suggestions_key, 86400 * 7, suggestions_bytes)  # 7ì¼ ë³´ê´€
